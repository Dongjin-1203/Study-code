{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNEaraMFh1mZp0DY00SW9HY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dongjin-1203/Study-code/blob/main/%EC%9E%84%EB%B2%A0%EB%94%A9_%EC%A0%95%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 임베딩 정리"
      ],
      "metadata": {
        "id": "i1ARpGaksrmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "XVvkaGLWR3ng",
        "outputId": "4b668898-28c6-440f-b84a-bfe2d24eb9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 임베딩\n",
        "\n",
        "앞서 배운 토큰화는 데이터를 컴퓨터가 읽을 수 있는 형태로 바꿔 준 것에 불구하고 컴퓨터가 텍스트를 이해하기 위해 텍스트를 숫자로 바꿔주는 __텍스트 벡터화__가 필요하다.\n",
        "\n",
        "기초적인 텍스트 벡터화에는 __원-핫 인코딩, 빈도 벡터화__등이 있다.\n",
        "\n",
        "원핫 인코딩은 문서에 등장하는 모든 단어를 고유 색인 값으로 매핑하고 색인 위치를 1, 나머지를 0으로 표현하는 방법이다.\n",
        "\n",
        "빈도 벡터화는 문서에 단어의 빈도수를 세어 해당 단어의 빈도를 벡터로 표현하는 방법이다. 예를 들어 apple이라는 단어가 6번 등장하면 벡터값을 6을 할당한다.\n",
        "\n",
        "원-핫 인코딩 방식은 여러번 등장하더라도 벡터가 1,0으로 표현되는 반면 빈도 벡터화는 그렇지 않다.\n",
        "\n",
        "이런 방식은 쉽고 간단하긴 하나 벡터의 희소성이 크다는 단점이 있다\n",
        "\n",
        "예를 들어 텍스트 벡터가 입력 텍스트의 의미를 내포하지 않고 있어 두 문장이 의미적으로 유사하더라도 벡터는 아닐 수 있고 원-핫 인코딩과 빈도벡터가 같은 경우, 토큰 개수만큼 벡터 차원을 가져야 하는데 문장 내에 존재하는 토큰의 수가 떨어져 컴퓨팅 비용의 증가나 차원의 저주같은 문제가 생길 수 있다.\n",
        "\n",
        "이러한 문제가 있어 __워드 임베딩__이란 기법을 사용하는 것이다.\n",
        "\n",
        "워드 임베딩은 단어를 고정된 길이의 실수 벡터로 표현하는 방법으로, 단어의 의미를 벡터 공간에서 다른 언어와의 상대적 위치로 표현해 단어간의 관계를 추론한다.\n",
        "\n",
        "워드 임베딩은 고정된 임베딩을 학습하기에 다의어나 문맥 정보를 다루기 어렵다는 단점이 있어 인공 신경망을 활용해 동적 임베딩 기법을 사용한다."
      ],
      "metadata": {
        "id": "s8wbFlq7e95q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 언어 모델\n",
        "\n",
        "언어 모델이란 입력된 문장으로 각 문장을 생성할 수 있는 확률을 계산하는 모델을 말한다.\n",
        "\n",
        "언어 모델은 자동 번역, 음성 인식, 텍스트 요약 등 다양한 자연어 처리 분야에서 활용된다."
      ],
      "metadata": {
        "id": "DVXmQt91ldMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 자기회귀 언어 모델\n",
        "\n",
        "입력된 문장들의 조건부 확률을 이용해 다음에 올 단어를 예측한다. 언어 모델에서 조건부 확률은 이전 단어들의 시퀀스가 주어졌을 때, 다음 단어의 확률을 계산하는 것을 의미한다.\n",
        "\n",
        "\n",
        "언어 모델의 조건부 확률\n",
        "\n",
        "$P(w_t|w_1, w_2, w_3, \\cdots , w_{t-1}) = \\frac{P(w_1, w_2, \\cdots , w_t)}{P(w_1, w_2, \\cdots , w_{t-1})}$\n",
        "\n",
        "언어 모델에서 조건부 확률을 계산하기 위해 이전에 등장한 단어 시퀀스를 기반으로 다음 단어의 확률을 계산한다. 위 수식에 연쇄법칙을 적용하면 아래 식이 나온다.\n",
        "\n",
        "$P(w_t|w_1, w_2, w_3, \\cdots , w_{t-1}) = P(w_1)P(w_2|w_1)\\cdots P(w_t|w_1, w_2, \\cdots , w_{t-1})$\n",
        "\n",
        "자기회귀 언어 모델에서는 각 시점에서 다음에 올 토큰을 예측하는 것이 중요하다. 이를 위해 입력 토큰 $w_1$을 바탕으로 모델은 다음토큰이 등장할 확률을 계산하고 그 다음모델의 출력값을 입력값으로 받아 다음 토큰이 출력할 값을 계산한다.\n",
        "\n",
        "이런 방식으로 출력값이 입력값으로 사용되는 특징 때문에 자기회귀라는 이름이 붙었다."
      ],
      "metadata": {
        "id": "P-TnXxbIl3wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 통계적 언어 모델\n",
        "\n",
        "통계적 언어 모델은 언어의 통계적 구조를 이용해 문장이나 단어의 시퀀스를 생성하거나 분석한다. 시퀀스에 대한 확률분포를 추정하여 문장의 문맥을 파악해 다음에 등장할 단어의 확률을 예측한다.\n",
        "\n",
        "일반적으로 마르코프 체인을 이용해 구현 된다. 마르코프 체인은 빈도 기반으 ㅣ조건부 확률 모델 중 하나로 이전 상태와 현재 상태 간의 전이 확률을 이용해 다음 상태를 예측한다.\n",
        "\n",
        "빈도 기반의 조건부 확률 모델에서는 주어진 데이터에서 각 변수가 발생한 빈도수를 기반으로 확률을 계산한다. 예를들어 말뭉치에 안녕하세요라는 문장이 1000번, 안녕하세요 만나서가 700번, 안녕하세요 반갑습니다가 100번 등장한다 가정하면 그 확률은 아래와 같다.\n",
        "\n",
        "$P(만나서|안녕하세요) = \\frac{P(안녕하세요 만나서)}{P(안녕하세요)} = \\frac{700}{1000}$\n",
        "\n",
        "$P(반갑습니다|안녕하세요) = \\frac{P(안녕하세요 반갑습니다)}{P(안녕하세요)} = \\frac{700}{1000}$\n",
        "\n",
        "이 방법은 단어의 순서와 빈도에만 기초해 문장의 확률을 예측하므로 문맥을 제대로 파악하지 못하면 불완전하거나 부적절한 결과를 생성할 수 있다. 또한 한 번도 등장한 적이 없는 단어나 문장에 대해서는 정확한 확률을 예측하기 어렵다.(데이터 희소성)\n",
        "\n",
        "하지만 통계적 언어 모델은 기존에 학습한 텍스트 데이터에서 패턴을 찾아 확률 분보를 생성하므로, 이를 이용해 새로운 문장을 생성할 수 있으며, 다양한 종류의 텍스트 데어를 학습할 수 있다.\n",
        "\n",
        "통계적 언어 모델은 대규모 자연어 데이터를 처리하는 데 효과적이며, 딥러닝 등의 인공지능 기술이 발전하며 더욱 강력한 모델을 구현할 수 있게 되었다.\n",
        "\n",
        "최근 연구되는 자연어 처리 기법은 언어 모델을 활용해 가중치를 사전 학습한다. 대표적으로 GPT, BERT가 있다."
      ],
      "metadata": {
        "id": "ZTC0tOlxs_RN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-gram\n",
        "\n",
        "가장 기초적인 통계적 언어모델이다. N-gram은 텍스트에서 N개의 연속된 단어 시퀀스를 하나의 단위로 취급하여 특정 단어 시퀀스가 등장할 확률을 추정한다.\n",
        "\n",
        "N-gram모델은 입력 텍스트를 하나의 토큰 단위로 분석하지 않고 N개의 토큰을 묶어서 분석한다. 이때, 연속된 N개의 단어를 하나의 단위로 취급하여 추론하는 모델이며 N이 1개일때 유니그램, 2는 바이그램, 3은 트라이그램이라 부른다. (4이상은 N-gram이라고 한다.)\n",
        "\n",
        "N-gram 언어 모델은 모든 토큰을 사용하지 않고 N-1개의 토큰만을 고려해 확률을 계산한다. 수식은 아래와 같다.\n",
        "\n",
        "$P(w_t|w_{t-1}, w_{t-2}, \\cdots , w_{t-N+1})$\n",
        "\n",
        "**예제1 N-gram**"
      ],
      "metadata": {
        "id": "nujVytDWz5_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "def ngrams(sentence, n):\n",
        "    words = sentence.split()\n",
        "    ngrams = zip(*[words[i:] for i in range(n)])\n",
        "    return list(ngrams)\n",
        "\n",
        "sentence = \"안녕하세요 만나서 진심으로 반가워요\"\n",
        "\n",
        "# 순수 파이썬 코드로 구현한 N-gram\n",
        "unigram = ngrams(sentence, 1)\n",
        "bigram = ngrams(sentence, 2)\n",
        "trigram = ngrams(sentence, 3)\n",
        "\n",
        "print(unigram)\n",
        "print(bigram)\n",
        "print(trigram)\n",
        "\n",
        "# NLTK 사용\n",
        "unigram = nltk.ngrams(sentence.split(), 1)\n",
        "bigram = nltk.ngrams(sentence.split(), 2)\n",
        "trigram = nltk.ngrams(sentence.split(), 3)\n",
        "\n",
        "print(list(unigram))\n",
        "print(list(bigram))\n",
        "print(list(trigram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPtKzTwc2YUT",
        "outputId": "8075ee6c-2a1d-4ee1-96e1-f3c519a7d9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
            "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
            "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n",
            "[('안녕하세요',), ('만나서',), ('진심으로',), ('반가워요',)]\n",
            "[('안녕하세요', '만나서'), ('만나서', '진심으로'), ('진심으로', '반가워요')]\n",
            "[('안녕하세요', '만나서', '진심으로'), ('만나서', '진심으로', '반가워요')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N-gram은 NLTK에서 지원하며 파이썬으로 직접 구현한 것과 동일한 결과물이 출력된 것을 볼 수 있다.\n",
        "\n",
        "N-gram은 작은 규모의 데이터셋에서 연속된 문자열 패턴을 분석하는데 큰 효과를 보인다.\n",
        "\n",
        "N-gram은 시퀀스에서 연속된 n개의 단어를 추출하므로 단어의 순서가 중요한 자연어 처리 작업 및 문자열 패턴 분석에 활용된다."
      ],
      "metadata": {
        "id": "zVFhD7V3dwk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF_IDF\n",
        "\n",
        "TF-IDF는 텍스트 문서에서 특정 단어의 중요도를 계산하는 방법으로 문서 내에서 단어의 중요도를 평가하는데 사용되는 통계적인 __가중치__를 의미한다. 즉, BoW(문서나 문장을 단어집합으로 표현하는 방법으로, 문서나 문장에 등장하는 단어의 중복을 허용해 빈도를 기록한다.)에 가중치를 부여하는 방법이다."
      ],
      "metadata": {
        "id": "54XZsxu2emNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 단어 빈도\n",
        "\n",
        "단어 빈도는 문서 내에서 특정단어의 빈도수를 나타내는 값이다.\n",
        "\n",
        "$TF(t,d) = count(t,d)$\n",
        "\n",
        "BoW 벡터 표현방법에서 처럼 문서내에서 단어가 등장한 빈도수를 계산하며 해당 단어의 상대적인 중요도를 측정하는데 사용된다. 많이 등장할수록 중요 단어로 인식 될 수 있지만, 특정 문서에서 자주 사용되는 단어라면 전문 용어나 관용어로 간주할 수 있다. 또는 문서 길이가 단순히 길어지면 TF값이 높아질 수 있다."
      ],
      "metadata": {
        "id": "qPbjEya7foji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 문서 빈도\n",
        "\n",
        "문서 빈도는 한 단어가 얼마나 많은 문서에 나타나는지를 의미한다. 특정 단어가 많은 문서에 나타나면 문서 집합에서 단어가 나타나는 횟수를 계산한다.\n",
        "\n",
        "$DF(t,D) = count(t \\in d:d \\in D)$\n",
        "\n",
        "DF는 단어가 몇 개의 문서에서 등장하는지 계산한다. DF 값이 높으면 특정 단어가 많은 문서에서 등장한다고 볼 수 있다. 그 단어는 일반적으로 널리 사용되며, 중요도가 낮을 수 있다."
      ],
      "metadata": {
        "id": "K7JMAdP6g5c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 역문서 빈도\n",
        "\n",
        "역문서 빈도는 전체 문서 수를 문서 빈도로 나눈 다음에 로그를 취한 값을 말한다. 이는 문서 내에서 특정 단어가 얼마나 중요한지를 나타낸다. 문서 빈도가 높을수록 해당 단어가 일반적이고 상대적으로 중요하지 않다는 의미가 된다. 그래서 문서 빈도(DF)의 역수를 취하면 단어의 빈도수가 적을수록 IDF가 커지게 보정하는 역할을 한다.\n",
        "\n",
        "$IDF(t,D) = log(\\frac{count(D)}{1+DF(t,D)})$\n",
        "\n",
        "분모에 1을 더하는 이유는 만약 어떤 단어가 한번도 등장하지 않는다면 DF는 0이 되며 분모가 0이되는 경우가 발생한다. 이를 방지하기 위해 1을 더한 것이다.\n",
        "로그를 취하므로서 너무 큰값이 나오는 것을 방지한다."
      ],
      "metadata": {
        "id": "zWJQ7krniPMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TF-IDF\n",
        "\n",
        "TF-IDF는 앞선 내용에서 TF와 IDF를 곱한값을 사용한다.\n",
        "\n",
        "$TF-IDF(t,d,D) = TF(t,d) \\times IDF(t,D)$\n",
        "\n",
        "문서 내에 단어가 자주 등장하지만 전체 문서 내에 해당단어가 적게 등장하면 TF-IDF는 커진다. 그래서 전체 문서에 자주 등장할 확률이 있는 관사나 관용어는 가중치가 낮아진다."
      ],
      "metadata": {
        "id": "hI8aSodgj6fV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF는 사이킷런에서 클래스 제공을해준다.\n",
        "\n",
        "**TF-IDF 클래스**\n",
        "\n",
        "```\n",
        "tfidf_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(\n",
        "    input = \"content\"   # 문자열 또는 바이트 형태 입력값. 파일명 가능\n",
        "    encoding = \"utf-8\",\n",
        "    lowercase = True,   # 소문자 변환\n",
        "    stop_words = None,  # 불용어 처리, 등록된 불용어는 사전에 등록되지 않는다.\n",
        "    ngram_range = (1, 1),   # N-gram 범위 지정(최소, 최대)\n",
        "    max_df = 1.0,    # max_df 가 정수면 해당 등장 횟수를 초과하는 단어를, 1이하 실수이면 해당 비율을 초과하는 단어를 불용어처리를 한다.\n",
        "    min_df = 1,    # 위에 내용과 반대\n",
        "    vocabulary = None,  # 미리 구축한 단어 사전 등록\n",
        "    smooth_idf = True   # IDF 계산시 분모에 1을 더한다.\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "MEPtUlznmBXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제2 TF-IDF 계산**"
      ],
      "metadata": {
        "id": "PQafgCnFsaYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    \"That movie is famous movie\",\n",
        "    \"I like that actor\",\n",
        "    \"I don’t like that actor\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(corpus)\n",
        "tfidf_matrix = tfidf_vectorizer.transform(corpus)\n",
        "# tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(tfidf_matrix.toarray())\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rx5JqKZmPU_",
        "outputId": "99a6b13f-f899-4bae-825b-0b3b9aaa826c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.39687454 0.39687454 0.         0.79374908\n",
            "  0.2344005 ]\n",
            " [0.61980538 0.         0.         0.         0.61980538 0.\n",
            "  0.48133417]\n",
            " [0.4804584  0.63174505 0.         0.         0.4804584  0.\n",
            "  0.37311881]]\n",
            "{'that': 6, 'movie': 5, 'is': 3, 'famous': 2, 'like': 4, 'actor': 0, 'don': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word2Vec\n",
        "\n",
        "Word2Vec는 2013년 구글에서 공개한 임베딩 모델로 단어간의 유사성을 측정하기 위해 분포 가설을 기반으로 개발했다.\n",
        "\n",
        "분포 가설이란 같은 문맥에서 함께 자주 나타나는 단어들은 서로 유사한 의미를 가질 가능성이 높다는 가정이다. 분포 가설은 단어 간의 동시 발생 확률 분포를 이용해 단어 간의 유사성을 측정한다.\n",
        "\n",
        "예를 들어 '내일 자동차를 타고 부산에 간다'와 '내일 비행기를 타고 부산에 간다'라는 두 문장에서 자동차와 비행기는 주변에 분포한 단어들이 동일하거나 유사하므로 비슷한 의미를 가질 것이라고 예상한다.\n",
        "\n",
        "이러한 가정을 통해 단어의 분산 표현(단어를 고차원 벡터 공간에 매핑하여 단어의 의미를 담는 것)을 학습할 수 있다.\n",
        "\n",
        "분포 가설에 따라 단어의 의미는 문맥상 분포적 특성을 통해 나타난다. 즉, 유사한 문맥에서 등장하는 단어는 비슷한 벡터 공간상 위치를 갖는다.\n",
        "\n",
        "이러한 방법으로 빈도 기반의 벡터화 기법에서 발생했던 단어의 의미 정보를 저장하지 못하는 한계를 극복했으며, 대량의 텍스트 데이터에서 단어 간의 관계를 파악하고 벡터 공간상에서 유사한 단어를 군집화해 단어의 의미 정보를 효과적으로 표현했다."
      ],
      "metadata": {
        "id": "JxZx_wmXtNHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 단어 벡터화\n",
        "\n",
        "단어 벡터화에는 희소 표현과 밀집 표현이 있다. 희소표현에는 원-핫 인코딩, TF-IDF등의 빈도 기반 방법이 해당되고, Word2Vec은 밀집 표현에 해당된다.\n",
        "\n",
        "밀집표현은 단어를 고정된 크기의 실수 벡터로 표현하기 때문에 단어 사전의 크기가 커지더라도 벡터의 크기가 커지지 않는다. 벡터 공간상에서 단어 간의 거리를 효과적으로 계산할 수 있으며, 벡터의 대부분이 0이 아닌 실수로 이뤄져 효율적으로 공간을 활용할 수 있다.\n",
        "\n",
        "희소 표현으로 벡터 요소를 표현한다 해보자. 만일 5000개의 단어가 사전에 있고 이를 10개의 토큰으로 표현하게 되면 최소 4999개의 0이 포함된 벡터가 나올 것이고 굉장한 낭비이다.\n",
        "\n",
        "이에 비해 밀집 표현으로 벡터를 표현하면 단어사전이 커지더라도 벡터의 크기는 커지지 않아 효율적이다.\n",
        "\n",
        "또한 밀집 표현 벡터화는 학습을 통해 단어를 벡터화 하기에 단어의 의미를 비교할 수 있다. 밀집 표현된 벡터를 단어 임베딩 벡터라고 한다."
      ],
      "metadata": {
        "id": "IpnV-WUNzBnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CBoW\n",
        "\n",
        "CBoW는 주변에 있는 단어를 가지고 중간에 있는 단어를 예측하는 방법이다. 예측해야하는 단어를 중심 단어, 예측에 이용되는 단어를 주변 단어라고 한다.\n",
        "\n",
        "중심 단어를 예측하기 위해 볌위를 정해줘야하는데 이를 윈도(window)라고 한다.\n",
        "\n",
        "학습을 위해 윈도를 이동해 가며 학습하는데 이를 슬라이딩 윈도라고 한다.\n",
        "\n",
        "CBoW의 학습데이터는 (주변 단어 | 중심 단어)로 구성된다. CBoW 모델은 각 입력 단어의 원-핫 벡터를 입력값으로 받고 입력 문장의 모든 단어의 임베딩 벡터를 평균내어 중심 단어의 임베딩 벡터를 예측한다.\n",
        "\n",
        "https://reniew.github.io/21/ 링크에 단어 임베딩과 Word2Vec, CBoW에 관한 정리와 다이어그램이 있으니 참고하자."
      ],
      "metadata": {
        "id": "2gJz-3zpY1jE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-gram\n",
        "\n",
        "Skip-gram은 CBoW와 반대로 중심 단어를 입력으로 받아 주변단어를 예측하는 모델이다. 따라서 Skip-gram은 중심 단어를 기준으로 양쪽으로 윈도 크기만큼 단어들을 주변 단어로 삼아 훈련 데이터셋을 만든다.\n",
        "\n",
        "이때 중심단어와 각 주변 단어를 하나의 쌍으로 하여 모델을 학습시킨다. 그래서 CBoW와 달리 한 윈도에서 여러개의 학습데이터가 만들어진다. 그래서 더 많은 학습 데이터를 추출할 수 있고 CBoW보다 뛰어난 성능을 보인다.\n",
        "\n",
        "https://www.blog.data101.io/353 링크에 Skip-gram의 데이터 생성 과정에대해 잘 설명 되어있다.\n",
        "\n",
        "**실습**\n",
        "\n",
        "Word2Vec 모델은 학습할 단어의 수를 V로, 임베딩 차원을 E로 설정해 $W_{V\\times E}$행렬과 $W_{V\\times E}^\\prime$행렬을 최적화하며 학습한다. 이때, $W_{V\\times E}$행렬은 __룩업(배열이나 리스트 등의 데이터 구조에서 인덱스를 이용해 해당하는 값을 찾아오는 연산을 의미.)__연산을 수행하는데, __임베딩__클래스를 사용하면 간편하게 구현할 수 있다.\n",
        "\n",
        "임베딩 클래스는 단어나 범주형 변수와 같은 __이산 변수를 연속적인 벡터 형태로 변환__(룩업)해 사용할 수 있다. 연속적인 벡터 표현은 모델이 학습하는 동안 단어의 의미와 관련된 정보를 포착하고, 이를 기반으로 단어 간의 유사도를 계산한다.\n",
        "\n",
        "```\n",
        "embedding = torch.nn.Embedding(\n",
        "    num_embeddings,  # 이산 변수의 개수로 단어 사전의 크기\n",
        "    embedding_dim,  # 임베딩 벡터의 차원 수로 임베딩 벡터의 크기\n",
        "    padding_idx=None,  # 패딩 토큰의 인덱스를 지정해 해당 인덱스의 임베딩 벡터를 0으로 설정함.\n",
        "    # 병렬 처리를 위해 입력 배치의 문장 길이가 동일해야 하므로 입력 문장들을 일정한 길이로 맞추는 역할을 한다.\n",
        "    # 패딩 인덱스는 임베딩 수보다 작아야 하며 패딩 인덱스의 벡터값은 모델 학습 시 최적화 되지 않음.\n",
        "    max_norm=None,  # 임베딩 벡터의 최대 크기 지정. 임베딩 벡터 크기가 norm값 보다 크면 잘라내고 크기를 줄인다.\n",
        "    norm_type=2.0  # 임베깅 벡터 크기를 제한하는 방법을 선택(기본: 2 - L2 정규화, 1 - L1 정규화)\n",
        ")\n",
        "```\n",
        "\n",
        "**예제3 기본 Skip-gram 클래스**"
      ],
      "metadata": {
        "id": "Jw3DKmKFlMwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class VanillaSkipgram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings = vocab_size,\n",
        "            embedding_dim = embedding_dim\n",
        "        )\n",
        "        self.linear = nn.Linear(\n",
        "            in_features = embedding_dim,\n",
        "            out_features = vocab_size\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeddings = self.embedding(input_ids) # Removed the extra comma here\n",
        "        output = self.linear(embeddings)\n",
        "        return output"
      ],
      "metadata": {
        "id": "3u6sN3MOS8gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제4 영화 리뷰데이터세트 전처리**"
      ],
      "metadata": {
        "id": "BImQ_oP4VfwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece Korpora"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "RzvHhRcqV46W",
        "outputId": "f5758ff8-0ff6-4aa2-a74b-263ee1062e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting Korpora\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting dataclasses>=0.6 (from Korpora)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2025.8.3)\n",
            "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: dataclasses, Korpora\n",
            "Successfully installed Korpora-0.2.0 dataclasses-0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              },
              "id": "d71148b5320045d7bdeaa69e75cc813e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uef0ksMFWNaF",
        "outputId": "9cc23a9f-1fa0-476a-abcb-77a8012b33ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (496 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.6/496.6 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 코퍼스 선언\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)\n",
        "\n",
        "# 토큰화\n",
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eZP3h6wVl0Z",
        "outputId": "0b527b6e-693e-4c77-844c-f4b2f53dc068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n",
            "[['굳', 'ㅋ'], ['GDNTOPCLASSINTHECLUB'], ['뭐', '야', '이', '평점', '들', '은', '....', '나쁘진', '않지만', '10', '점', '짜', '리', '는', '더', '더욱', '아니잖아']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제5 단어 사전 구축**"
      ],
      "metadata": {
        "id": "Hpgq5xoEYsSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 단어 사전 구축\n",
        "def build_vocab(corpus, n_vocab, special_tokens):\n",
        "    counter = Counter()     # 토큰 개수 확인\n",
        "    for tokens in corpus:\n",
        "        counter.update(tokens)\n",
        "    vocab = special_tokens\n",
        "    for token, count in counter.most_common(n_vocab):\n",
        "        vocab.append(token)\n",
        "    return vocab\n",
        "\n",
        "vocab = build_vocab(corpus=tokens, n_vocab=5000, special_tokens=[\"<unk>\"])      # 단어 사전에 없는 단어를 <unk>토큰으로 대체하기 위해 special_tokens를 선언했다.\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "id_to_token = {idx: token for idx, token in enumerate(vocab)}\n",
        "\n",
        "print(vocab[:10])\n",
        "print(len(vocab))   # 단어사전 길이5000에 특수 토큰1개 까지 해서 5001개"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTi4_yKvYw8f",
        "outputId": "5299bcc9-3027-4aaf-eee4-0119d37325bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '.', '이', '영화', '의', '..', '가', '에', '...', '을']\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제6 Skip-gram 단어 쌍 추출**"
      ],
      "metadata": {
        "id": "6nFymtdPg_BC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 쌍 추출\n",
        "def get_word_pairs(tokens, window_size):\n",
        "    pairs = []\n",
        "    for sentence in tokens:\n",
        "        sentence_length = len(sentence)\n",
        "        for idx, center_word in enumerate(sentence):\n",
        "            # 탐색 구간 지정: 문장의 경계를 넘지 않도록 주의\n",
        "            window_start = max(0, idx - window_size)\n",
        "            window_end = min(sentence_length, idx + window_size + 1)\n",
        "            center_word = sentence[idx]\n",
        "            context_words = sentence[window_start:idx] + sentence[idx+1:window_end]\n",
        "            for context_word in context_words:\n",
        "                pairs.append([center_word, context_word])\n",
        "    return pairs\n",
        "\n",
        "word_pairs = get_word_pairs(tokens, window_size=2)\n",
        "print(word_pairs[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KplEJieQhMjm",
        "outputId": "f956ed4d-62b2-410f-dba1-49c1da49808f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['굳', 'ㅋ'], ['ㅋ', '굳'], ['뭐', '야'], ['뭐', '이'], ['야', '뭐']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제7 인덱스 쌍 변환**"
      ],
      "metadata": {
        "id": "AipnUz4tuUWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index_pairs(word_pairs, token_to_id):\n",
        "    pairs = []\n",
        "    unk_index = token_to_id[\"<unk>\"]\n",
        "    for word_pair in word_pairs:\n",
        "        center_word, context_word = word_pair\n",
        "        center_index = token_to_id.get(center_word, unk_index)\n",
        "        context_index = token_to_id.get(context_word, unk_index)\n",
        "        pairs.append([center_index, context_index])\n",
        "    return pairs\n",
        "\n",
        "\n",
        "index_pairs = get_index_pairs(word_pairs, token_to_id)\n",
        "print(index_pairs[:5])\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1t3w2FmxZkT",
        "outputId": "b0c2b081-a8a6-4f60-d392-77804e4cf2cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[595, 100], [100, 595], [77, 176], [77, 2], [176, 77]]\n",
            "5001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제8 데이터로더 적용**"
      ],
      "metadata": {
        "id": "apqD2jzExeoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# 단어쌍을 텐서로 변환\n",
        "index_pairs = torch.tensor(index_pairs)\n",
        "center_indexes = index_pairs[:, 0]\n",
        "context_indexes = index_pairs[:, 1]\n",
        "\n",
        "dataset = TensorDataset(center_indexes, context_indexes)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "cG-SURHRxkO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제9 Skip-gram 모델 준비**"
      ],
      "metadata": {
        "id": "EVBO42VXx3hs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "word2vec = VanillaSkipgram(vocab_size=len(token_to_id), embedding_dim=128).to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = optim.SGD(word2vec.parameters(), lr=0.1)"
      ],
      "metadata": {
        "id": "tbmQ_KEcyKAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제10 모델 학습**"
      ],
      "metadata": {
        "id": "NqjzaE4KyPN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    cost = 0.0\n",
        "    for input_ids, target_ids in dataloader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        target_ids = target_ids.to(device)\n",
        "\n",
        "        logits = word2vec(input_ids)\n",
        "        loss = criterion(logits, target_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        cost += loss\n",
        "\n",
        "    cost = cost / len(dataloader)\n",
        "    print(f\"Epoch : {epoch+1:4d}, Cost : {cost:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOyp1i2_ySDU",
        "outputId": "68d9ba53-1872-4429-df81-b06a13729b23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :    1, Cost : 6.196\n",
            "Epoch :    2, Cost : 5.981\n",
            "Epoch :    3, Cost : 5.931\n",
            "Epoch :    4, Cost : 5.901\n",
            "Epoch :    5, Cost : 5.879\n",
            "Epoch :    6, Cost : 5.862\n",
            "Epoch :    7, Cost : 5.847\n",
            "Epoch :    8, Cost : 5.834\n",
            "Epoch :    9, Cost : 5.822\n",
            "Epoch :   10, Cost : 5.812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제11 임베딩 값 추출**"
      ],
      "metadata": {
        "id": "Z7484ery3tFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_embedding = dict()\n",
        "embedding_matrix = word2vec.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "for word, embedding in zip(vocab, embedding_matrix):\n",
        "    token_to_embedding[word] = embedding\n",
        "\n",
        "index = 30\n",
        "token = vocab[index]\n",
        "token_embedding = token_to_embedding[token]\n",
        "print(token)\n",
        "print(token_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ucs-xGpV3wZO",
        "outputId": "44f04523-f635-4854-9750-84a401833f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "연기\n",
            "[-0.3138322   0.63234013 -0.8135999   0.16403873  0.4248039  -1.3998375\n",
            " -1.3413075  -0.14470072  0.9595877  -0.5902127  -1.486693    0.27782288\n",
            " -0.1221083  -0.24955271  0.07255645 -0.270792    1.238364    0.30257654\n",
            " -0.94090194 -0.87884295  0.4468749  -1.1813976  -0.13524853  0.14081746\n",
            " -1.2610563   1.2052584  -0.41063243 -0.7307844   0.06623446 -0.3573198\n",
            "  0.9784512   0.5872654  -2.515568   -0.81870353 -0.46787933  0.6167379\n",
            " -0.08659018  1.2215097  -0.6403556   0.5171093   0.2573679   0.19807912\n",
            " -1.0309634   0.18601812  1.9397566   0.05055966  0.40003866  0.6649197\n",
            " -0.8819643  -0.22857802  0.78639525  0.37067065  0.4270564   0.03185772\n",
            "  0.761853   -0.06979126  1.2305583  -0.08378909 -0.59683555  0.39305493\n",
            "  2.0753527  -0.87213486 -1.1068225  -0.6683612   0.33620107 -0.9223261\n",
            " -0.88533217 -0.5728379   1.0592022  -2.1923528  -0.0728803   0.2327228\n",
            " -0.6766303  -1.2041185  -0.81515944 -0.88956976  2.0172758   0.2560349\n",
            "  0.41021937 -0.11889981  1.566716   -0.41204304 -0.99185395 -0.7589987\n",
            " -0.35112193 -0.07764686 -1.101672    0.5830342   0.91504955 -0.7901901\n",
            "  0.1102566  -0.24339005 -0.7105819  -0.31844404  0.49776018  1.4271389\n",
            " -0.62729925  0.20639636 -0.03735858  0.77617246 -0.23725694  0.4484059\n",
            " -1.1797713   0.87968093 -2.406843    0.03744636  0.123044   -0.12660687\n",
            " -0.31919852  0.4838855  -1.0310022   0.55143416 -0.7382107  -1.2700803\n",
            " -0.95684236 -1.0178951   0.4156357  -0.90069383 -1.3785707  -1.2172978\n",
            " -0.02213333  0.66038376  0.5102426   0.22334604 -0.10531351 -0.30493426\n",
            "  2.6628666  -0.83406705]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "코사인 유사도를 통해 임베딩의 유사도를 측정한다."
      ],
      "metadata": {
        "id": "KcMSCSxl36kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제12 단어 임베딩 유사도 계산**"
      ],
      "metadata": {
        "id": "-8YzMpXh4Ove"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    cosine = np.dot(b, a) / (norm(b, axis=1) * norm(a))\n",
        "    return cosine\n",
        "\n",
        "def top_n_index(cosine_matrix, n):\n",
        "    closest_indexes = cosine_matrix.argsort()[::-1]\n",
        "    top_n = closest_indexes[1 : n + 1]\n",
        "    return top_n\n",
        "\n",
        "\n",
        "cosine_matrix = cosine_similarity(token_embedding, embedding_matrix)\n",
        "top_n = top_n_index(cosine_matrix, n=5)\n",
        "\n",
        "print(f\"{token}와 가장 유사한 5 개 단어\")\n",
        "for index in top_n:\n",
        "    print(f\"{id_to_token[index]} - 유사도 : {cosine_matrix[index]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZK7DYNh4TS5",
        "outputId": "5be938b0-bd19-4187-d773-0c61039f8a30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "연기와 가장 유사한 5 개 단어\n",
            "이뻐 - 유사도 : 0.2866\n",
            "다른 - 유사도 : 0.2803\n",
            "보이 - 유사도 : 0.2730\n",
            "배우 - 유사도 : 0.2727\n",
            "없다는게 - 유사도 : 0.2697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 계층적 소프트맥스\n",
        "\n",
        "계층적 소프트맥스는 출력층을 이진 트리구조로 표현해 연산을 수행한다. 이때 자주 등장하는 단어일수록 트리의 상위 노드에 위치하고, 드물게 등장하는 단어 일수록 하위 노드에 배치된다.\n",
        "\n",
        "머신러닝 트리모델과 유사한 구조이고 각 노드는 학습이 가능한 벡터를 가지며, 입력값은 해당 노드의 벡터의 내적값을 계산한 후 시그모이드 함수를 통해 확률을 계산한다.\n",
        "\n",
        "https://uponthesky.tistory.com/15 참고 링크"
      ],
      "metadata": {
        "id": "m50ARkszm01c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 네거티브 샘플링\n",
        "\n",
        "네거티브 샘플링은 Word2Vec 모델에서 사용되는 확률적인 샘플링 기법으로 전체 단어 집합에서 일부단어를 샘플링하여 오답단어로 사용한다.\n",
        "\n",
        "학습 윈도에서 등장하지 않은 단어 N(5~20개)개를 추출하여 정답 단어와 소프트맥스 연산을 수행한다.\n",
        "\n",
        "$P(w_i)=\\frac{f(W_i)^{0.75}}{\\sum_{j=0}^V f(W_i)^{0.75}}$\n",
        "\n",
        "위 수식대로 계산이 진행되며 $w_i$출현 빈도를 계산할 단어이다. $P(w_i)$는 단어 $w_i$가 네거티브 샘플로 추출될 확률이다. 이때 0.75제곱은 정규화 상수이고 이는 실험을 통해 얻어진 값이다.\n",
        "\n",
        "네거티브 샘플링에서는 입력 단어 쌍이 데이터로부터 추출된 단어 쌍인지, 아니면 네거티브 샘플링으로 생성된 단어 쌍인지 이진분류를 한다. 분류모델은 로지스틱 회귀모델이고 학습과정에서 추출할 단어의 확률 분포를 구하기 위해 먼저 각 단어에 대한 가중치를 학습한다.\n",
        "\n",
        "https://woochan-autobiography.tistory.com/564 이 링크에서 네거티브 샘플링이 어떻게 되는것인지, Word2Vec모델에서 출력이 어떻게 되는지 정리가 되어있다.\n",
        "\n",
        "네거티브 샘플링을 하면 이진분류모델(데이터셋에서 나온 훈련 데이터냐 아니냐)로 학습 목적이 바뀐다. Word2Vec모델은 입력 단어의 임베딩과 샘플이 맞는지 여부를 나타내는 레이블을 내적 연산하고 그 값을 시그모에드 함수를 통해 확률로 변환을 한다."
      ],
      "metadata": {
        "id": "p1C3FNlUpOiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 실습: Gensim\n",
        "\n",
        "매우 간단한 구조인 Word2Vec 모델을 학습할 때 데이터 수가 적은 경우에도 학습에는 오랜 시간이 걸린다. 이런 경우 계층적 소프트맥스나 네거티브 샘플링 같은 기법을 사용하면 더 효율적이다.\n",
        "\n",
        "Gensim 라이브러리를 활용하면 Word2Vec과 같은 NLP 모델을 쉽게 구성할 수 있다. 젠심 라이브러리는 대용량 텍스트 데이터의 처리를 위한 메모리 효율적인 방법을 제공해 대규모 데이터 세트에서도 효과적인 모델을 학습할 수 있다.\n",
        "\n",
        "또한 학습된 모델을 저장하여 관리할 수 있고, 비슷한 단어 찾기 등 유사도와 관련된 기능도 제공하여 자연어 처리에 필요한 다양한 기능을 제공한다.\n",
        "\n",
        "**Gensim 설치**\n",
        "```\n",
        "!pip install gensim\n",
        "```\n",
        "\n",
        "젠심은 사이썬을 이용해 병렬 처리나 네거티브 샘플링 등을 적용하고, 젠심으로 파이 토치를 이용한 학습보다 빠른 속도로 학습할 수 있다.\n",
        "\n",
        "```\n",
        "word2vec = gensim.models.Word2Vec(\n",
        "    sentences = None,  # 학습 데이터\n",
        "    corpus_file = None,  # 학습데이터 파일 경로\n",
        "    vector_size = 100,  # 임베딩 벡터 크기\n",
        "    alpha = 0.025,  # 학습률\n",
        "    window = 5,  # 학습의 윈도우 크기\n",
        "    sg = 0,  # Skip-gram 사용여부(1: Skip-gram, 0: CBoW)\n",
        "    hs = 0,  # 계층적 소프트맥수 사용여부(1: 사용, 0: 안사용)\n",
        "    cbow_mean = 1,  # 단어 합 벡터의 평균화 여부(1: 사용, 0: 안사용)\n",
        "    negative = 5,  # 네거티브 샘플링 개수\n",
        "    ns_exponent = 0.75,  # 네거티브 샘플링 확률의 지수\n",
        "    max_final_vocab = None   # 단어 사전의 최대 크기를 의미한다. 최소 빈도를 충족하는 단어가 최대 최종 단어 사전보다 많으면 자주 등장한 단어 순으로 단어사전을 구축\n",
        "    epoch = 5,\n",
        "    batch_words = 10000\n",
        ")\n",
        "```\n",
        "**예제13 Word2Vec 모델 학습**"
      ],
      "metadata": {
        "id": "Ym8u5-Revrim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece Korpora konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "scCi3cvY5_TQ",
        "outputId": "f882d06e-b157-4f19-d36b-7a763193c37e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting Korpora\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting dataclasses>=0.6 (from Korpora)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2025.8.3)\n",
            "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Downloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (496 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.6/496.6 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dataclasses, JPype1, Korpora, konlpy\n",
            "Successfully installed JPype1-1.6.0 Korpora-0.2.0 dataclasses-0.6 konlpy-0.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              },
              "id": "fa7e52668dce48bdb8da1f7553cc2dc3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "EznDv7f39DAK",
        "outputId": "10770792-6a50-4087-e85b-a1b0c4b4b31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "454113fe7146441bb07931c37726ea5b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from Korpora import Korpora\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "# 코퍼스 생성 / 데이터 프레임 변환\n",
        "corpus = Korpora.load(\"nsmc\")\n",
        "corpus = pd.DataFrame(corpus.test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OabpHb2o530P",
        "outputId": "763541ff-15bf-492d-b117-8c925d3d060d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : e9t@github\n",
            "    Repository : https://github.com/e9t/nsmc\n",
            "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
            "\n",
            "    Naver sentiment movie corpus v1.0\n",
            "    This is a movie review dataset in the Korean language.\n",
            "    Reviews were scraped from Naver Movies.\n",
            "\n",
            "    The dataset construction is based on the method noted in\n",
            "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
            "\n",
            "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_train.txt\n",
            "[Korpora] Corpus `nsmc` is already installed at /root/Korpora/nsmc/ratings_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 토큰화\n",
        "tokenizer = Okt()\n",
        "tokens = [tokenizer.morphs(review) for review in corpus.text]"
      ],
      "metadata": {
        "id": "EiLokP_n6RNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "word2vec = Word2Vec(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    epochs=3,\n",
        "    max_final_vocab=10000\n",
        ")"
      ],
      "metadata": {
        "id": "jigpp_ib89Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제14 임베딩 추출 및 유사도 계산**"
      ],
      "metadata": {
        "id": "0B5-dunX-gkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"연기\"\n",
        "print(word2vec.wv[word])    # 단어 벡터와 유사도 계산 결과\n",
        "print(word2vec.wv.most_similar(word, topn=5))       # 유사 단어 5개 추출\n",
        "print(word2vec.wv.similarity(w1=word, w2=\"연기력\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0rYEeYS-krZ",
        "outputId": "5380c825-6f34-4f2f-a84f-5c977507d308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-9.31754485e-02 -2.61156052e-01  3.30786079e-01  3.00796032e-01\n",
            "  7.46272728e-02  1.81755394e-01  7.04306737e-02  8.55713151e-03\n",
            " -7.38197505e-01  3.99832755e-01 -1.05741814e-01 -3.06390263e-02\n",
            " -2.43785977e-01  1.16714716e-01 -2.71429956e-01 -2.61985958e-01\n",
            " -1.61808357e-01  1.00326003e-03 -2.53671914e-01  4.21295643e-01\n",
            "  6.42553210e-01  3.04849565e-01 -2.89477021e-01 -1.31662488e-01\n",
            " -3.74423563e-01 -7.53450841e-02 -3.47370505e-01  5.17008156e-02\n",
            "  1.62640512e-01 -1.71329483e-01 -4.11011577e-01  2.33955383e-01\n",
            "  4.25195724e-01 -2.26906061e-01  3.08662802e-01 -9.47081670e-02\n",
            " -1.65524647e-01  8.16064551e-02 -3.38523000e-01 -4.41862345e-01\n",
            " -1.42158747e-01  8.79149586e-02 -2.47466996e-01 -5.44641435e-01\n",
            " -3.71472359e-01 -7.37818927e-02 -3.03122163e-01 -1.25919461e-01\n",
            "  7.73998126e-02 -3.85045707e-02  6.91615462e-01  1.52505785e-01\n",
            "  1.06794097e-01  4.93691536e-04 -4.45258647e-01 -1.45596102e-01\n",
            "  1.51602164e-01  1.62849545e-01 -1.19639650e-01  2.62113184e-01\n",
            "  1.59097537e-01 -3.22440892e-01  8.77594277e-02 -7.12543586e-03\n",
            " -3.08365375e-01  3.06584954e-01 -6.35515377e-02  4.78463173e-01\n",
            "  4.67728227e-01 -2.56684542e-01 -2.77940899e-01 -1.22227399e-02\n",
            " -3.83507788e-01  3.50687504e-02 -7.99387097e-02  1.92388073e-02\n",
            " -2.85987377e-01 -1.52020782e-01 -4.42564249e-01  4.09177810e-01\n",
            "  9.15561393e-02  2.10071817e-01  5.15833199e-01  8.30883265e-01\n",
            "  1.58144116e-01  4.58217412e-01  1.10314973e-01 -4.20585960e-01\n",
            " -1.15415312e-01 -1.70027599e-01 -3.00698727e-01 -2.82126784e-01\n",
            "  3.00214440e-01  1.76696464e-01  4.86525953e-01 -1.72618404e-01\n",
            " -1.30204707e-01  8.28899071e-02 -2.04070106e-01 -3.51663113e-01\n",
            " -1.75626457e-01  9.91489142e-02  2.80868739e-01 -2.57391870e-01\n",
            " -8.13270584e-02  9.47918743e-02  3.23832095e-01  1.89431623e-01\n",
            "  1.21237569e-01 -5.15487850e-01  3.29255879e-01 -1.53200343e-01\n",
            " -2.66001821e-01 -1.33239981e-02  2.18096524e-01 -2.07278877e-01\n",
            "  5.07080317e-01  4.13927436e-01 -1.29987165e-01  2.45495245e-01\n",
            "  5.92647009e-02 -2.17402264e-01  1.34641781e-01 -1.16171509e-01\n",
            " -5.24265841e-02  1.00037955e-01 -5.90035915e-01 -1.79194659e-01]\n",
            "[('연기력', 0.7612825036048889), ('연기자', 0.7244047522544861), ('캐스팅', 0.7123674154281616), ('조연', 0.7110012769699097), ('여배우', 0.7076730132102966)]\n",
            "0.76128256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec은 분포 가설을 통해 쉽고 빠르게 단어의 임베딩을 학습할 수 있지만, 단어의 형태학적 특징을 반영하지 못한다는 한계가 있다.\n",
        "\n",
        "특히 한국어 같은 교착어 학습이 제한되며 학습시 많은 OOV를 발생시킬 수 있다."
      ],
      "metadata": {
        "id": "bA6wKMcUAtgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fastText\n",
        "\n",
        "fastText는 2015년 메타에서 개발한 오픈소스 임베딩 모델로, 텍스트 분류 및 텍스트 마이닝을 위한 알고리즘이다. fastText는 단어와 문장을 벡터로 변환하는 기술을 기반으로 하며, 이를 통해 머신러닝 알고리즘이 텍스트 데이터를 분석하고 이해하는데 사용된다.\n",
        "\n",
        "Word2Vec과 차이는 fastText는 단어의 하위 단어를 고려해 더 높은 정확도와 성능을 제공한다. 이때문에 N-gram을 사용해 단어를 분해하고 벡터화 한다.\n",
        "\n",
        "fastText에서는 단어의 벡터화를 위해 <. >과 같은 특수기호를 사용해 단어의 시작과 끝을 나타낸다.\n",
        "\n",
        "기호가 추가된 단어는 N-gram을 위해 하위 단어 집합으로 분해 된다.\n",
        "\n",
        "**예시**    \n",
        "'서울특별시' n=2인 바이그램으로 분해    \n",
        " => '서울', '울특', '특별', '별시'\n",
        "\n",
        " 분해된 하위 단어 집합에는 나눠지지 않은 단어 자신도 포함되며, 단어 집합이 만들어지면 각 하위 단어는 고유한 벡터값을 갖게 된다.\n",
        "\n",
        " 이러한 하위 단어 벡터들은 단어의 벡터 표현을 구성하며, 이를 사용하여 자연어 처리 작업을 수행한다.\n",
        "\n",
        " 일반적으로 fastText는 다양한 N-gram을 적용해 입력 토큰을 분해하고 하위 단어 벡터를 구성함으로써 단어의 부분 문자열을 고려하는 유연하고 정확한 하위 단어 집합을 생성한다.\n",
        "\n",
        " 즉, 같은 하위 단어를 공유하는 단어끼리는 정보를 공유해 학습하며 비슷한 단어끼리는 비슷한 임베딩 벡터를 갖게 되고 단어 간 유사도를 높일 수 있다. 그리고 OOV단어도 하위 단어로 나눠 임베딩을 계산할 수 있다.\n",
        "\n",
        " 이렇게 하위 단어 기반의 임베딩 방법을 사용하면, 말뭉치에 등장하지 않은 단어라도 유사한 하위 단어를 가지고 있으면 유사한 임베딩 벡터를 갖게 된다.\n",
        "\n",
        " **FastText 클래스**\n",
        " ```\n",
        " fasttext = gensim.models.FastText(\n",
        "    sentences = None,\n",
        "    corpus_file = None,\n",
        "    vector_size = 100,\n",
        "    alpha = 0.025,\n",
        "    window = 5,\n",
        "    min_count = 5,\n",
        "    workers = 3,\n",
        "    sg = 0,\n",
        "    hs = 0,\n",
        "    cbow_mean = 1,\n",
        "    negative = 5,\n",
        "    ns_exponent = 0.75,\n",
        "    max_final_vocab = None,\n",
        "    epoch = 5,\n",
        "    batch_words = 10000,\n",
        "    min_n = 3,  # 사용할 N-gram의 최소값\n",
        "    max_n = 6  # 사용할 N-gram의 최대값\n",
        " )\n",
        " ```\n",
        " 나머지는 Word2Vec의 파라미터와 같다."
      ],
      "metadata": {
        "id": "NM6C0mG-GOD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제15 KoNLI 데이터세트 전처리**"
      ],
      "metadata": {
        "id": "KF93Tkkabn-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "corpus = Korpora.load(\"kornli\")\n",
        "corpus_texts = corpus.get_all_texts() + corpus.get_all_pairs()\n",
        "# get_all_texts(): 문장을 튜플로\n",
        "# get_all_pairs():  입력 문장과 쌍을 이루는 대응 문장을 튜플 형태로 반환\n",
        "tokens = [sentence.split() for sentence in corpus_texts]\n",
        "\n",
        "print(tokens[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOykGBEsbdEe",
        "outputId": "4f6e766f-89e2-40b3-e3a5-f4ec3e4d4084"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : KakaoBrain\n",
            "    Repository : https://github.com/kakaobrain/KorNLUDatasets\n",
            "    References :\n",
            "        - Ham, J., Choe, Y. J., Park, K., Choi, I., & Soh, H. (2020). KorNLI and KorSTS: New Benchmark\n",
            "           Datasets for Korean Natural Language Understanding. arXiv preprint arXiv:2004.03289.\n",
            "           (https://arxiv.org/abs/2004.03289)\n",
            "\n",
            "    This is the dataset repository for our paper\n",
            "    \"KorNLI and KorSTS: New Benchmark Datasets for Korean Natural Language Understanding.\"\n",
            "    (https://arxiv.org/abs/2004.03289)\n",
            "    We introduce KorNLI and KorSTS, which are NLI and STS datasets in Korean.\n",
            "\n",
            "    # License\n",
            "    Creative Commons Attribution-ShareAlike license (CC BY-SA 4.0)\n",
            "    Details in https://creativecommons.org/licenses/by-sa/4.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[kornli] download multinli.train.ko.tsv: 83.6MB [00:02, 40.4MB/s]                            \n",
            "[kornli] download snli_1.0_train.ko.tsv: 78.5MB [00:00, 248MB/s]                            \n",
            "[kornli] download xnli.dev.ko.tsv: 516kB [00:00, 8.12MB/s]\n",
            "[kornli] download xnli.test.ko.tsv: 1.04MB [00:00, 14.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['개념적으로', '크림', '스키밍은', '제품과', '지리라는', '두', '가지', '기본', '차원을', '가지고', '있다.'], ['시즌', '중에', '알고', '있는', '거', '알아?', '네', '레벨에서', '다음', '레벨로', '잃어버리는', '거야', '브레이브스가', '모팀을', '떠올리기로', '결정하면', '브레이브스가', '트리플', 'A에서', '한', '남자를', '떠올리기로', '결정하면', '더블', 'A가', '그를', '대신하러', '올라가고', 'A', '한', '명이', '그를', '대신하러', '올라간다.'], ['우리', '번호', '중', '하나가', '당신의', '지시를', '세밀하게', '수행할', '것이다.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KoNLI데이터 셋은 자연어 추론을 위한 데이터셋이다.\n",
        "* 자연어 추론: 두 개 이상의 문장이 주어졌을때, 두 문장 간의 관계를 분류하는 작업을 말함.\n",
        "\n",
        "이를 통해 주어진 문장이 함의 관계, 중립 관계, 불일치 관계 중 어느 관계에 해당되는지 분류할 수 있다."
      ],
      "metadata": {
        "id": "aPjVHfojgL8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제16 fastText 모델 실습**"
      ],
      "metadata": {
        "id": "kn7o7aZpi9hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "\n",
        "fastText = FastText(\n",
        "    sentences=tokens,\n",
        "    vector_size=128,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,\n",
        "    max_final_vocab=20000,\n",
        "    epochs=3,\n",
        "    min_n=2,\n",
        "    max_n=6\n",
        ")"
      ],
      "metadata": {
        "id": "SmiSwq56i8-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장\n",
        "# fastText.save(\"../models/fastText.model\")\n",
        "# fastText = FastText.load(\"../models/fastText.model\")"
      ],
      "metadata": {
        "id": "HjhRBwmZjJCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제17 fastText OOV 처리**"
      ],
      "metadata": {
        "id": "148kCHnMjvZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oov_token = \"사랑해요\"\n",
        "oov_vector = fastText.wv[oov_token]\n",
        "\n",
        "print(oov_token in fastText.wv.index_to_key)\n",
        "# wv.index_to_key: 학습된 단어 사전을 나타내는 리스트\n",
        "print(fastText.wv.most_similar(oov_vector, topn=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lmZYm9bj0vF",
        "outputId": "b8824353-b116-4cbc-a76b-29664a8128a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "[('사랑', 0.8846595883369446), ('사랑에', 0.8430585265159607), ('사랑의', 0.7911879420280457), ('사랑을', 0.7782157063484192), ('사랑하는', 0.7682992219924927)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 사전 리스트에는 '사랑해요'라는 단어가 없어 OOV토큰으로 처리가된다.\n",
        "\n",
        "Word2Vec 모델은 단어 사전에 존재하지 않는 단어는 임베딩을 계산할 수 없었다. 하지만 fastText는 하위 단어로 나눠 있기에 단어를 하위단어로 분해하고 하위 단어의 임베딩을 모두 합해서 전체 토큰의 임베딩을 계산한다. 이런식으로 OOV문제를해결할 수 있다."
      ],
      "metadata": {
        "id": "jWni-UaikAnk"
      }
    }
  ]
}