# 공부자료 업로드

## 2025.08.13
### 임베딩 Skip-gram 실습
- Skip-gram은 CBoW와 반대로 중심 단어를 입력으로 받아 주변단어를 예측하는 모델이다. 따라서 Skip-gram은 중심 단어를 기준으로 양쪽으로 윈도 크기만큼 단어들을 주변 단어로 삼아 훈련 데이터셋을 만든다.
  
- 이때 중심단어와 각 주변 단어를 하나의 쌍으로 하여 모델을 학습시킨다. 그래서 CBoW와 달리 한 윈도에서 여러개의 학습데이터가 만들어진다. 그래서 더 많은 학습 데이터를 추출할 수 있고 CBoW보다 뛰어난 성능을 보인다.

Word2Vec 모델은 학습할 단어의 수를 V로, 임베딩 차원을 E로 설정해 $W_{V\times E}$행렬과 $W_{V\times E}^\prime$행렬을 최적화하며 학습한다. 이때, $W_{V\times E}$행렬은 __룩업(배열이나 리스트 등의 데이터 구조에서 인덱스를 이용해 해당하는 값을 찾아오는 연산을 의미.)__연산을 수행하는데, __임베딩__클래스를 사용하면 간편하게 구현할 수 있다.
임베딩 클래스는 단어나 범주형 변수와 같은 __이산 변수를 연속적인 벡터 형태로 변환__(룩업)해 사용할 수 있다. 연속적인 벡터 표현은 모델이 학습하는 동안 단어의 의미와 관련된 정보를 포착하고, 이를 기반으로 단어 간의 유사도를 계산한다.
```
embedding = torch.nn.Embedding(
    num_embeddings,  # 이산 변수의 개수로 단어 사전의 크기
    embedding_dim,  # 임베딩 벡터의 차원 수로 임베딩 벡터의 크기
    padding_idx=None,  # 패딩 토큰의 인덱스를 지정해 해당 인덱스의 임베딩 벡터를 0으로 설정함. 
    # 병렬 처리를 위해 입력 배치의 문장 길이가 동일해야 하므로 입력 문장들을 일정한 길이로 맞추는 역할을 한다.
    # 패딩 인덱스는 임베딩 수보다 작아야 하며 패딩 인덱스의 벡터값은 모델 학습 시 최적화 되지 않음.
    max_norm=None,  # 임베딩 벡터의 최대 크기 지정. 임베딩 벡터 크기가 norm값 보다 크면 잘라내고 크기를 줄인다.
    norm_type=2.0  # 임베깅 벡터 크기를 제한하는 방법을 선택(기본: 2 - L2 정규화, 1 - L1 정규화)
)
```
-----------------------------------
## 2025.08.14
### 임베딩 Gensim 학습 및 Word2Vec활용 Gensim 실습
매우 간단한 구조인 Word2Vec 모델을 학습할 때 데이터 수가 적은 경우에도 학습에는 오랜 시간이 걸린다. 이런 경우 계층적 소프트맥스나 네거티브 샘플링 같은 기법을 사용하면 더 효율적이다.
Gensim 라이브러리를 활용하면 Word2Vec과 같은 NLP 모델을 쉽게 구성할 수 있다. 젠심 라이브러리는 대용량 텍스트 데이터의 처리를 위한 메모리 효율적인 방법을 제공해 대규모 데이터 세트에서도 효과적인 모델을 학습할 수 있다.
또한 학습된 모델을 저장하여 관리할 수 있고, 비슷한 단어 찾기 등 유사도와 관련된 기능도 제공하여 자연어 처리에 필요한 다양한 기능을 제공한다.

**Gensim 설치**
```
!pip install gensim
```

젠심은 사이썬을 이용해 병렬 처리나 네거티브 샘플링 등을 적용하고, 젠심으로 파이 토치를 이용한 학습보다 빠른 속도로 학습할 수 있다.

**Word2Vec 클래스 구성**
```
word2vec = gensim.models.Word2Vec(
    sentences = None,  # 학습 데이터
    corpus_file = None,  # 학습데이터 파일 경로
    vector_size = 100,  # 임베딩 벡터 크기
    alpha = 0.025,  # 학습률
    window = 5,  # 학습의 윈도우 크기
    sg = 0,  # Skip-gram 사용여부(1: Skip-gram, 0: CBoW)
    hs = 0,  # 계층적 소프트맥수 사용여부(1: 사용, 0: 안사용)
    cbow_mean = 1,  # 단어 합 벡터의 평균화 여부(1: 사용, 0: 안사용)
    negative = 5,  # 네거티브 샘플링 개수
    ns_exponent = 0.75,  # 네거티브 샘플링 확률의 지수
    max_final_vocab = None   # 단어 사전의 최대 크기를 의미한다. 최소 빈도를 충족하는 단어가 최대 최종 단어 사전보다 많으면 자주 등장한 단어 순으로 단어사전을 구축
    epoch = 5,
    batch_words = 10000
)
```
