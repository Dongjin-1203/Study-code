# 공부자료 업로드

## 2025.08.13
### 임베딩 Skip-gram 실습
- Skip-gram은 CBoW와 반대로 중심 단어를 입력으로 받아 주변단어를 예측하는 모델이다. 따라서 Skip-gram은 중심 단어를 기준으로 양쪽으로 윈도 크기만큼 단어들을 주변 단어로 삼아 훈련 데이터셋을 만든다.
  
- 이때 중심단어와 각 주변 단어를 하나의 쌍으로 하여 모델을 학습시킨다. 그래서 CBoW와 달리 한 윈도에서 여러개의 학습데이터가 만들어진다. 그래서 더 많은 학습 데이터를 추출할 수 있고 CBoW보다 뛰어난 성능을 보인다.

Word2Vec 모델은 학습할 단어의 수를 V로, 임베딩 차원을 E로 설정해 $W_{V\times E}$행렬과 $W_{V\times E}^\prime$행렬을 최적화하며 학습한다. 이때, $W_{V\times E}$행렬은 __룩업(배열이나 리스트 등의 데이터 구조에서 인덱스를 이용해 해당하는 값을 찾아오는 연산을 의미.)__연산을 수행하는데, __임베딩__클래스를 사용하면 간편하게 구현할 수 있다.
임베딩 클래스는 단어나 범주형 변수와 같은 __이산 변수를 연속적인 벡터 형태로 변환__(룩업)해 사용할 수 있다. 연속적인 벡터 표현은 모델이 학습하는 동안 단어의 의미와 관련된 정보를 포착하고, 이를 기반으로 단어 간의 유사도를 계산한다.
```
embedding = torch.nn.Embedding(
    num_embeddings,  # 이산 변수의 개수로 단어 사전의 크기
    embedding_dim,  # 임베딩 벡터의 차원 수로 임베딩 벡터의 크기
    padding_idx=None,  # 패딩 토큰의 인덱스를 지정해 해당 인덱스의 임베딩 벡터를 0으로 설정함. 
    # 병렬 처리를 위해 입력 배치의 문장 길이가 동일해야 하므로 입력 문장들을 일정한 길이로 맞추는 역할을 한다.
    # 패딩 인덱스는 임베딩 수보다 작아야 하며 패딩 인덱스의 벡터값은 모델 학습 시 최적화 되지 않음.
    max_norm=None,  # 임베딩 벡터의 최대 크기 지정. 임베딩 벡터 크기가 norm값 보다 크면 잘라내고 크기를 줄인다.
    norm_type=2.0  # 임베깅 벡터 크기를 제한하는 방법을 선택(기본: 2 - L2 정규화, 1 - L1 정규화)
)
```
