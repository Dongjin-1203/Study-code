{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNujXYXm6/sgaDFx6V1D584",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dongjin-1203/Study-code/blob/main/%ED%86%A0%ED%81%B0%ED%99%94_%EC%A0%95%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토크화 정리"
      ],
      "metadata": {
        "id": "i1ARpGaksrmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVvkaGLWR3ng",
        "outputId": "3743cb7d-1b6e-4521-dc0f-ef50e9fc318f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단어 및 글자 토큰화\n",
        "\n",
        "토큰화는 자연어 처리에서 매우 중요한 전처리 과정으로 텍스트 데이터를 구조적으로 분해하여 개별 토큰으로 나누는 작업을 의미한다.\n",
        "토큰화 과정은 정확한 분석을 위해 필수이며, 단어나 문장의 빈도수, 출현 패턴 등을 파악할 수 있다.\n",
        "\n",
        "또한 작은 단위로 분해된 텍스트 데이터는 컴퓨터가 이하하고 처리하기 용이하다.\n",
        "\n",
        "토큰화는 텍스트 데이터럴 __단어 / 글자 단위로 나누는 기법__으로 나눌 수 있다."
      ],
      "metadata": {
        "id": "2lMNhPUuswAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 단어 토큰화\n",
        "\n",
        "__단어 토큰화__는 자연어 처리 분야에서 핵심적인 전처리 작업 중 하나로 텍스트 데이터를 의미 있는 단위인 단어로 분리하는 작업이다.\n",
        "\n",
        "단어 단위로 나누기에 띄어쓰기, 문장부호를 기준으로 나뉘며 이를바탕으로 문장을 이해한다.\n",
        "단어 토큰화는 품사 태깅, 개체명 인식, 기계 번역 등의 작업에서 널리 사용되며 가장 일반적인 방법이다."
      ],
      "metadata": {
        "id": "0gF9hr7DtBkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제1 단어 토큰화**"
      ],
      "metadata": {
        "id": "oQ1chTRBv_80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoRa6G7msjcU",
        "outputId": "968a2de6-7325-4bc9-bf31-5ac4cc857f54"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['현실과',\n",
              " '구분',\n",
              " '불가능한',\n",
              " 'cg.',\n",
              " '시각적',\n",
              " '즐거움은',\n",
              " '최고!',\n",
              " '더불어',\n",
              " 'ost는',\n",
              " '더더욱',\n",
              " '최고!!']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "review = \"현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!\"\n",
        "tokenized = review.split()\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 출력 결과처럼 공백을 기준으로 나뉘는 것을 알 수 있다. 만약 특정기준으로 나누고 싶다면, 구분자를 입력해주면 된다.\n",
        "\n",
        "그리고 위 토큰을 보면 '최고!'가 있고 '최고!!'가 있다. 이는 뜻은 같지만 다른 토큰이다. __따라서 단어 토큰화는 한국어 접사, 문장부호, 오타 또는 띄어쓰기 오류에 취약하다.__"
      ],
      "metadata": {
        "id": "LOy1FPz4wqSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 글자 토큰화\n",
        "__글자 토큰화__는 띄어쓰기뿐만 아니라 글자 단위로 문장을 나누는 방식으로, 비교적 작은 단어 사전을 구축할 수 있다는 장점이 있다. 작은 단어 사전을 사용하면 학습 시 컴퓨터 자원을 아낄 수 있으며, 전체 말뭉치를 학습할 때 각 단어를 더 자주 학습할 수 있다는 장점이 있다.\n",
        "\n",
        "글자 토큰화는 언어 모델링과 같은 시퀀스 예측작업에서 활용된다. 여를 들어 다음 문자를 예측하는 언어 모델링에서 글자 토큰화는 유용한 방식이다."
      ],
      "metadata": {
        "id": "XqowmT58xhj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제2 글자 토큰화**"
      ],
      "metadata": {
        "id": "eueC2i4ayc23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!\"\n",
        "tokenized = list(review)\n",
        "tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFb5RdmGwlui",
        "outputId": "96341bfe-c08a-419b-afea-079893b5909d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['현',\n",
              " '실',\n",
              " '과',\n",
              " ' ',\n",
              " '구',\n",
              " '분',\n",
              " ' ',\n",
              " '불',\n",
              " '가',\n",
              " '능',\n",
              " '한',\n",
              " ' ',\n",
              " 'c',\n",
              " 'g',\n",
              " '.',\n",
              " ' ',\n",
              " '시',\n",
              " '각',\n",
              " '적',\n",
              " ' ',\n",
              " '즐',\n",
              " '거',\n",
              " '움',\n",
              " '은',\n",
              " ' ',\n",
              " '최',\n",
              " '고',\n",
              " '!',\n",
              " ' ',\n",
              " '더',\n",
              " '불',\n",
              " '어',\n",
              " ' ',\n",
              " 'o',\n",
              " 's',\n",
              " 't',\n",
              " '는',\n",
              " ' ',\n",
              " '더',\n",
              " '더',\n",
              " '욱',\n",
              " ' ',\n",
              " '최',\n",
              " '고',\n",
              " '!',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 토큰화와 다르게 공백도 토큰화가 진행되며 영어에경우 각 알파벳으로 나뉜다.\n",
        "\n",
        "하지만 한글은 하나에 글자에 여러 자음과 모음의 조합으로 이뤄져 있어 __자소 단위 토큰화__가 필요하다."
      ],
      "metadata": {
        "id": "rSoTyZxGyp3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jamo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph01cvZeVzgG",
        "outputId": "60416e2e-63a4-4c34-eafa-e89c215d1d8b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jamo\n",
            "  Downloading jamo-0.4.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: jamo\n",
            "Successfully installed jamo-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제3 자모(jamo) 라이브러리**"
      ],
      "metadata": {
        "id": "9h4bpXgCz0sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jamo import h2j, j2hcj\n",
        "\n",
        "\n",
        "review = \"현실과 구분 불가능한 cg. 시각적 즐거음은 최고! 더불어 ost는 더더욱 최고!!\"\n",
        "decomposed = j2hcj(h2j(review))\n",
        "tokenized = list(decomposed)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzcd9kwSyoZo",
        "outputId": "0d2b5142-4b8e-4889-e9b4-fed467ac719e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅡ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위와 같이 개별 토큰은 아무 의미도 가지지 못하며 자연어 모델이 각 토큰의 의미를 조합해 결과를 도출해야한다!"
      ],
      "metadata": {
        "id": "yk5EVLCi0efW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 형태소 토큰화\n",
        "\n",
        "__형태소 토큰화__는 텍스트를 형태소 단위로 나누는 토큰화 방법이고 언어의 문법과 구조를 고려해 단어를 분리하고 이를 의미 있는 단위로 분류하는 작업이다.\n",
        "\n",
        "형태소 토큰화는 한국어와 같이 교착어(Agglutinative Language)인 언어에서 중요하게 수행된다."
      ],
      "metadata": {
        "id": "hVFR-3ar1GZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 형태소 어휘 사전\n",
        "\n",
        "__형태소 어휘 사전__은 자연어 처리에서 사용되는 단어의 집합인 어휘 사전 중에서도 각 단어의 형태소 정보를 포함하는 사전을 말한다. 이는 형태소 분석 작업에서 중요한 역할을 한다.\n",
        "\n",
        "텍스트 데이터를 형태소 분석하여 각 형태소에 해당하는 품사(Part Of Speech POS)를 태깅하는 작업을 POS Tagging이라고 한다. 이를 통해 자연어 처리 분야에서 문맥을 고려할 수 있어 더욱 정확한 분석이 가능해진다."
      ],
      "metadata": {
        "id": "7rVvdn8H1Mkz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KoNLPy\n",
        "\n",
        "KoNLPy는 한국어 자연어 처리를 위해 개발된 라이브러리로 명사 추출, 형태소 분석, 품사 태깅 등의 기능을 제공한다.\n",
        "\n",
        "**설치 방법**\n",
        "\n",
        "`pip install konlpy`\n",
        "\n",
        "KoNLPy는 JDK기반으로 개발 되었으며 __Okt, 꼬꼬마, 코모란, 한나눔, 메캅__ 등의  형태소 분석기를 지원한다."
      ],
      "metadata": {
        "id": "upI88TBB1QBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제4 Okt 토큰화**"
      ],
      "metadata": {
        "id": "DCAiNvDQ4Pxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "CILna60VTzk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc6e9ff-28af-48ac-b254-2ea737354707"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.11/dist-packages (from konlpy) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.11/dist-packages (from konlpy) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
            "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jpype1-1.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (496 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.6/496.6 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.6.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "\n",
        "\n",
        "okt = Okt()\n",
        "\n",
        "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
        "\n",
        "nouns = okt.nouns(sentence)\n",
        "phrases = okt.phrases(sentence)\n",
        "morphs = okt.morphs(sentence)\n",
        "pos = okt.pos(sentence)\n",
        "\n",
        "print(\"명사 추출 :\", nouns)\n",
        "print(\"구 추출 :\", phrases)\n",
        "print(\"형태소 추출 :\", morphs)\n",
        "print(\"품사 태깅 :\", pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyMcRuRD4TwX",
        "outputId": "b650f215-723f-4a30-bba0-e4223dc4cbc8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "명사 추출 : ['무엇', '상상', '수', '사람', '무엇', '낼', '수']\n",
            "구 추출 : ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']\n",
            "형태소 추출 : ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']\n",
            "품사 태깅 : [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제5 꼬꼬마 토큰화**"
      ],
      "metadata": {
        "id": "uDLfjZsA4m6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "\n",
        "\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\"\n",
        "\n",
        "nouns = kkma.nouns(sentence)\n",
        "sentences = kkma.sentences(sentence)\n",
        "morphs = kkma.morphs(sentence)\n",
        "pos = kkma.pos(sentence)\n",
        "\n",
        "print(\"명사 추출 :\", nouns)\n",
        "print(\"문장 추출 :\", sentences)\n",
        "print(\"형태소 추출 :\", morphs)\n",
        "print(\"품사 태깅 :\", pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT863RL74sEM",
        "outputId": "5e1082ac-47b5-440b-9d0a-01f30090d250"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "명사 추출 : ['무엇', '상상', '수', '사람', '무엇']\n",
            "문장 추출 : ['무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.']\n",
            "형태소 추출 : ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']\n",
            "품사 태깅 : [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "두 분석기가 각기다른 결과를 보여주고 있다. 이렇듯, 각 상황이나 시스템에 맞는 형태소 분석기를 사용해야 한다."
      ],
      "metadata": {
        "id": "7BOkCLg043un"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__KoNLPy Docs__\n",
        "\n",
        "https://konlpy.org/ko/v0.6.0/morph/"
      ],
      "metadata": {
        "id": "iywuQku65Rwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLTK\n",
        "\n",
        "NLTK는 자연어 처리를 위해 개발된 라이브러리로, 토큰화, 형태소 분석, 구문 분석, 개체명 인식, 감성 분석 등과 같은 기능을 제공한다.\n",
        "\n",
        "주로 영어에 사용되었으나 프랑스어, 네덜란드어, 독일어 등과 같은 다양한 언어의 자연어 처리를 위한 데이터와 모델을 제공한다.\n",
        "\n",
        "**설치 방법**\n",
        "\n",
        "`pip install nltk`\n",
        "\n",
        "NLTK에도 여러 분석기가 존재한다. 대표적으로 __Punkt, Averaged Perceptron Tagger__ 두가지가 있고 이는 __트리뱅크__라는 대규모 영어 말뭉치를 기반으로 학습 되었다."
      ],
      "metadata": {
        "id": "jO8xq9dB1Wm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제6 패키지 및 모델 다운로드**"
      ],
      "metadata": {
        "id": "63NlklPB7Ggc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"punkt_tab\")  # 설치 해주지 않으면 LookupError가 날 수 있음\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\")     # 설치 해주지 않으면 LookupError가 날 수 있음"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JhSrPjF7Myh",
        "outputId": "d39d3dd1-3a57-4246-cc8e-d187d3887493"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제7 영문 토큰화**"
      ],
      "metadata": {
        "id": "gmNBtDPE7Tnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "word_tokens = tokenize.word_tokenize(sentence)\n",
        "sent_tokens = tokenize.sent_tokenize(sentence)\n",
        "\n",
        "print(word_tokens)\n",
        "print(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T5cxhmw7XaA",
        "outputId": "6e900122-2669-44c0-857a-40377c83d879"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']\n",
            "['Those who can imagine anything, can create the impossible.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "나누는 기준은 사전 학습 모델을 통해 토큰화 한다.\n",
        "\n",
        "`word_tokenize`는 공백을 기준으로 단어를 나누고, 구두점 등을 처리해 각각의 단어를 리스트화 한다.\n",
        "\n",
        "`sent_tokenize`는 구두점을 기준으로 문장을 분리하고 리스트로 반환한다."
      ],
      "metadata": {
        "id": "Ti8oaZaQ7wK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제8 영문 품사 태깅**"
      ],
      "metadata": {
        "id": "yyBMLp4Q8V6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tag\n",
        "from nltk import tokenize\n",
        "\n",
        "\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "\n",
        "word_tokens = tokenize.word_tokenize(sentence)\n",
        "pos = tag.pos_tag(word_tokens)\n",
        "\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPJVBdPc8aNd",
        "outputId": "bf99f66f-affd-41d5-ca3f-90a515f16b97"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imagine', 'VB'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pos_tag`는 토큰화된 문장에서 품사 태깅을 수행한다.\n",
        "\n",
        "품사 태깅을 수행하려면 토큰화 된 단어들이 들어가야하며 앞서 설치한 `averaged_perceptron_tagger`가 있어야하며, 혹여 LookupError시에는 `averaged_perceptron_tagger_eng`을 설치해주면 된다."
      ],
      "metadata": {
        "id": "fl0V9P-t8r2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## spaCy\n",
        "\n",
        "spaCy는 사이썬 기반으로 개발 된 오픈 소스 라이브러리이다. NLTK와 마찬가지로 자연어 처리를 위한 기능을 제공한다.\n",
        "\n",
        "spaCy는 NLTK와 달리 효율적인 처리 속도와 높은 정확도를 제공하는 것을 목표로 한다.\n",
        "\n",
        "그래서 NLTK보다 더 크고 복잡하고 더 많은 리소스를 요구한다.\n",
        "\n",
        "**spaCy**\n",
        "\n",
        "```\n",
        "pip install spacy    \n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "spaCy는 GPU 가속을 비롯해 24개 이상의 언어로 사전 학습된 모델을 제공한다. `en_core_web_sm`는 영어로 사전 학습된 모델이다.\n",
        "\n",
        "GPU 가속 및 다른 언어를 사용하고자 하면 https://spacy.io/usage에서 파이프라인을 설치할 수 있다."
      ],
      "metadata": {
        "id": "0MXwUFHN1caT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제9 spaCy 품사 태깅**"
      ],
      "metadata": {
        "id": "E4ilXXvI_mLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "sentence = \"Those who can imagine anything, can create the impossible.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "for token in doc:\n",
        "    print(f\"[{token.pos_:5} - {token.tag_:3}] : {token.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YF1QFVro_rq8",
        "outputId": "e1080ca5-e13d-4251-9131-8ae4cd54fb02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PRON  - DT ] : Those\n",
            "[PRON  - WP ] : who\n",
            "[AUX   - MD ] : can\n",
            "[VERB  - VB ] : imagine\n",
            "[PRON  - NN ] : anything\n",
            "[PUNCT - ,  ] : ,\n",
            "[AUX   - MD ] : can\n",
            "[VERB  - VB ] : create\n",
            "[DET   - DT ] : the\n",
            "[ADJ   - JJ ] : impossible\n",
            "[PUNCT - .  ] : .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy는 사전 학습된 모델을 기반으로 처리하기에 `.load()`를 통해 모델을 설정해야한다.\n",
        "\n",
        "위 코드에서 spaCy는 doc객체에 여러 token으로 결과를 저장했다. token은 기본 품사 속성, 세분화 품사 속성, 원본 텍스트 데이터, 토큰 사이의 공백을 포함하는 텍스트 데이터, 벡터, 벡터 노름(vector_norm)등의 속성이 포함 되어있다."
      ],
      "metadata": {
        "id": "En_GO7vZ_yO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 하위 단어 토큰화\n",
        "\n",
        "위에서 우리는 기본적인 문장 전처리 기술을 배웠다. 단어, 글자, 형태소 단위로 토큰화 하는 방법을 배웠지만, 인간의 언어는 시간과 시대에 따라 조금씩 변하고 특히 컴퓨터는 신조어나 고유어들에 약할 수 있다. 또한, 시스템을 분석하려는 텍스트 데이터는 컴퓨터, 스마트폰에 의해 생기는데 이는 오탈자가 있을 수 밖에 없다.이것은 모델의 토큰화를 방해한다.\n",
        "\n",
        "그래서 우리는 이를 해결할 방법 중 하나인 __하위 단어 토큰화__라는 것을 이용했다.\n",
        "하위 단어 토큰화는 단어의 길이를 줄이고 처리속도가 빨라질 뿐만아니라 OOV문제, 신조어, 은어, 고유어 등으로 인한 문제를 완화할 수 있다.\n",
        "\n",
        "하위 단어 토큰화 방법으로 바이트 페어 인코딩, 워드피스, 유니그램 모델 등이 있다."
      ],
      "metadata": {
        "id": "KzIkGEHo1iV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 바이트 페어 인코딩\n",
        "\n",
        "__바이트 페어 인코딩__은 다이그램 코딩이라고도 불리며 텍스트 데이터에서 가장 빈번하게 등장하는 글자 쌍의 조합을 찾아 부호화하는 압축 알고리즘으로 초기에는 데이터 압축을 위해 개발하였으나, 자연어 처리 분야에서 하위단어 토큰화를 위한 방법으로 사용된다.\n",
        "\n",
        "이 알고리즘은 연속된 글자 쌍이 더 이상 나타나지 않거나 정해진 어휘 사전 크기에 도달할 때까지 조합탐지화 부호화를 반복하며 이 과정에서 자주등장하는 단어는 하나의 토큰으로 토큰화 하고, 덜 등장하는 단어는 여러 토큰의 조합으로 표현된다.\n",
        "\n",
        "**예시**\n",
        "- 원문. abracadabra\n",
        "- Step #1: AracadaAra\n",
        "- Step #2: ABcadAB\n",
        "- Step #3: CcadC\n",
        "\n",
        "1단계에서 자주보이는 Ab 글자 쌍을 A로 치환, 2단계에서 ra를 B, 3단계에서 AB를 C로 치환하였다. 이후는 치환 가능한 글자 쌍이 없으므로 CcadC로 압축 된 것이다. 이를 토크나이저로써 바이트 페어 인코딩은 자주 등장하는 글자 쌍을 찾아 치환하는 대신 어휘 사전에 추가한다.\n",
        "\n",
        "이번에는 빈도를 계산한 사전과 어휘 사전이 있다 가정하자.\n",
        "\n",
        "- 빈도 사전: ('low', 5), ('lower', 2), ('newest', 6), ('widest', 3)\n",
        "- 어휘 사전: ['low', 'lower', 'newest', 'widest']\n",
        "\n",
        "이를 BPE로 재구성 하면 우선 글자단위로 다 나눠준다.\n",
        "\n",
        "- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o',' w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
        "\n",
        "빈도 사전을 기준으로 자주 등장한 글자 쌍을 찾는다. 이 경우에는 'e', 's'이다. 이를 es 쌍으로 추가한다.\n",
        "\n",
        "- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o',' w', 'e', 'r', 2), ('n', 'e', 'w', 'es', 't', 6), ('w', 'i', 'd', 'es', 't', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es']\n",
        "\n",
        "동일 과정을 반복한다. 다음으로 많이 등장하는 단어는 'es', 't'이다.\n",
        "\n",
        "- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o',' w', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est']\n",
        "\n",
        "계속해서 반복하면 다음과 같은 결과가 나온다.\n",
        "\n",
        "4번째\n",
        "- 빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low']\n",
        "\n",
        "5번째\n",
        "- 빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne']\n",
        "\n",
        "6번째\n",
        "- 빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne', 'new']\n",
        "\n",
        "7번째\n",
        "- 빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest']\n",
        "\n",
        "8번째\n",
        "- 빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest']\n",
        "\n",
        "9번째\n",
        "- 빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid']\n",
        "\n",
        "10번째\n",
        "- 빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid', 'widest']\n",
        "\n",
        "11번째\n",
        "- 빈도 사전: ('low', 5), ('low', 'e', 'r', 2), ('n', 'e', 'w', 'est', 6), ('w', 'i', 'd', 'est', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'es', 'est', 'lo', 'low', 'ne', 'new', 'newest', 'wi', 'wid', 'widest', 'lowe', 'lower']\n",
        "\n",
        "위와 같은 과정으로 토큰화가 진행되고, 추후 말뭉치에 없는 단어가 나오게 되면 기존 어휘 사전을 참고해 토큰화하여 추가한다."
      ],
      "metadata": {
        "id": "mj688b1F1myB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 센텐스피스\n",
        "\n",
        "센텐스피스와 코포라 라이브러리를 활용해 토크나이저를 학습해보자.\n",
        "\n",
        "센텐스피스는 구글에서 개발한 오픈소스 하위단어 토크나이저이다. BPE와 유사한 알고리즘을 이용해 입력 데이터를 토큰화 하고 단어 사전을 생성한다. 또한 워드피스, 유니코드 기반의 다양한 알고리즘을 지원하며 다양한 파라미터를 제공하여 세밀한 토크나이징이 가능하다.\n",
        "\n",
        "코포라는 국립 국어원이나 AI hub에서 제공하는 말뭉치 데이터를 쉽게 사용할 수 있게 제공하는 오픈소스 라이브러리이다.\n",
        "\n",
        "**센텐스피스, 코포라 설치**\n",
        "```\n",
        "pip install sentencepiece Korpora\n",
        "```"
      ],
      "metadata": {
        "id": "f3agh5ePO7dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece Korpora"
      ],
      "metadata": {
        "id": "NpLkgmgBVm35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "108ab288-332e-495d-bf90-335ea28fa3b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Collecting Korpora\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting dataclasses>=0.6 (from Korpora)\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.32.3)\n",
            "Requirement already satisfied: xlrd>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from Korpora) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20.0->Korpora) (2025.8.3)\n",
            "Downloading Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: dataclasses, Korpora\n",
            "Successfully installed Korpora-0.2.0 dataclasses-0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dataclasses"
                ]
              },
              "id": "090ea40422914ed2b1dceccb23d3f08e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제10 청와대 청원 데이터 다운로드**"
      ],
      "metadata": {
        "id": "BEUOiTExPygl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "corpus = Korpora.load(\"korean_petitions\")\n",
        "dataset = corpus.train\n",
        "petition = dataset[0]\n",
        "\n",
        "print(\"청원 시작일 :\", petition.begin)\n",
        "print(\"청원 종료일 :\", petition.end)\n",
        "print(\"청원 동의 수 :\", petition.num_agree)\n",
        "print(\"청원 범주 :\", petition.category)\n",
        "print(\"청원 제목 :\", petition.title)\n",
        "print(\"청원 본문 :\", petition.text[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5F-l8RaP50G",
        "outputId": "f5cb8187-f12d-48e4-e520-85b311fabba8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
            "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
            "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
            "    단 청원의 동의 개수는 수집됩니다.\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 7.35MB/s]                            \n",
            "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 86.8MB/s]                            \n",
            "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 78.7MB/s]                            \n",
            "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 168MB/s]                             \n",
            "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 160MB/s]                             \n",
            "[korean_petitions] download petitions_2018-01: 43.9MB [00:00, 166MB/s]                            \n",
            "[korean_petitions] download petitions_2018-02: 33.8MB [00:01, 32.7MB/s]                            \n",
            "[korean_petitions] download petitions_2018-03: 34.3MB [00:09, 3.71MB/s]                            \n",
            "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 211MB/s]                             \n",
            "[korean_petitions] download petitions_2018-05: 37.5MB [00:00, 183MB/s]                            \n",
            "[korean_petitions] download petitions_2018-06: 37.8MB [00:01, 33.4MB/s]                            \n",
            "[korean_petitions] download petitions_2018-07: 40.5MB [00:01, 29.6MB/s]                            \n",
            "[korean_petitions] download petitions_2018-08: 39.8MB [00:01, 33.7MB/s]                            \n",
            "[korean_petitions] download petitions_2018-09: 36.1MB [00:00, 149MB/s]                            \n",
            "[korean_petitions] download petitions_2018-10: 38.1MB [00:01, 32.7MB/s]                            \n",
            "[korean_petitions] download petitions_2018-11: 37.7MB [00:00, 222MB/s]                            \n",
            "[korean_petitions] download petitions_2018-12: 33.0MB [00:00, 176MB/s]                             \n",
            "[korean_petitions] download petitions_2019-01: 34.8MB [00:01, 32.2MB/s]                            \n",
            "[korean_petitions] download petitions_2019-02: 30.8MB [00:00, 157MB/s]                             \n",
            "[korean_petitions] download petitions_2019-03: 34.9MB [00:00, 176MB/s]                             \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "청원 시작일 : 2017-08-25\n",
            "청원 종료일 : 2017-09-24\n",
            "청원 동의 수 : 88\n",
            "청원 범주 : 육아/교육\n",
            "청원 제목 : 학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.\n",
            "청원 본문 : 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제11 학습 데이터셋 생성**"
      ],
      "metadata": {
        "id": "0XBxrV6oRE9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "import os\n",
        "\n",
        "corpus = Korpora.load(\"korean_petitions\")\n",
        "petitions = corpus.get_all_texts()\n",
        "\n",
        "# Create the 'datasets' directory if it doesn't exist\n",
        "os.makedirs(\"../datasets\", exist_ok=True)\n",
        "\n",
        "with open(\"../datasets/corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for petition in petitions:\n",
        "        f.write(petition + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyazxRwQRJP_",
        "outputId": "dea446c0-d89d-4c45-82db-a95cd00b29de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : Hyunjoong Kim lovit@github\n",
            "    Repository : https://github.com/lovit/petitions_archive\n",
            "    References :\n",
            "\n",
            "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
            "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
            "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
            "    단 청원의 동의 개수는 수집됩니다.\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-08\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-09\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-10\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-11\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2017-12\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-01\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-02\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-03\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-04\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-05\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-06\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-07\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-08\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-09\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-10\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-11\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2018-12\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-01\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-02\n",
            "[Korpora] Corpus `korean_petitions` is already installed at /root/Korpora/korean_petitions/petitions_2019-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제12 토크나이저 모델 학습**"
      ],
      "metadata": {
        "id": "D7mUwFO_Swsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceTrainer\n",
        "import os\n",
        "\n",
        "# Create the 'models' directory if it doesn't exist\n",
        "os.makedirs(\"../models\", exist_ok=True)\n",
        "\n",
        "SentencePieceTrainer.Train(\n",
        "    \"--input=../datasets/corpus.txt\\\n",
        "    --model_prefix=../models/petition_bpe\\\n",
        "    --vocab_size=8000 model_type=bpe\"\n",
        ")"
      ],
      "metadata": {
        "id": "Qr1j9RMIStXk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/google/sentencepiece 이곳에가면 SentencePieceTrainer의 다양한 파라미터를 확인할 수 있다.\n",
        "\n",
        "토크나이저 모델 학습이 완료가 되면 모델 파일과 단어 사전 파일이 생성된다.\n",
        "이 파일을 열면 어떻게 토큰화 되어있는지 확인할 수 있다.\n",
        "\n",
        "이제 토크나이저 모델과 어휘 사전 파일을 활용해 BPE 인코딩을 수행해보자."
      ],
      "metadata": {
        "id": "oZBPpILOZvCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제13 BPE 인코딩 토큰화**"
      ],
      "metadata": {
        "id": "ELazJKiEamj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "\n",
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(\"../models/petition_bpe.model\")\n",
        "\n",
        "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
        "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
        "\n",
        "tokenized_sentence = tokenizer.encode_as_pieces(sentence)   # encode_as_pieces: 문장 토큰화\n",
        "tokenized_sentences = tokenizer.encode_as_pieces(sentences)\n",
        "print(\"단일 문장 토큰화 :\", tokenized_sentence)\n",
        "print(\"여러 문장 토큰화 :\", tokenized_sentences)\n",
        "\n",
        "encoded_sentence = tokenizer.encode_as_ids(sentence)    # 토큰을 정수로 인코딩\n",
        "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
        "print(\"단일 문장 정수 인코딩 :\", encoded_sentence)\n",
        "print(\"여러 문장 정수 인코딩 :\", encoded_sentences)\n",
        "\n",
        "decode_ids = tokenizer.decode_ids(encoded_sentences)    # 문자열 데이터로 변환\n",
        "decode_pieces = tokenizer.decode_pieces(encoded_sentences)    # 문자열 데이터로 변환\n",
        "print(\"정수 인코딩에서 문장 변환 :\", decode_ids)\n",
        "print(\"하위 단어 토큰에서 문장 변환 :\", decode_pieces)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgjcL6sUarem",
        "outputId": "b6fcd198-6052-4965-ab31-c99e873a95d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단일 문장 토큰화 : ['▁안녕하세요', ',', '▁토', '크', '나', '이', '저', '가', '▁잘', '▁학', '습', '되었', '군요', '!']\n",
            "여러 문장 토큰화 : [['▁이렇게', '▁입', '력', '값을', '▁리', '스트', '로', '▁받아서'], ['▁쉽게', '▁토', '크', '나', '이', '저', '를', '▁사용할', '▁수', '▁있', '답니다']]\n",
            "단일 문장 정수 인코딩 : [667, 6553, 994, 6880, 6544, 6513, 6590, 6523, 161, 110, 6554, 872, 787, 6648]\n",
            "여러 문장 정수 인코딩 : [[372, 182, 6677, 4433, 1772, 1613, 6527, 4162], [1681, 994, 6880, 6544, 6513, 6590, 6536, 5852, 19, 5, 2639]]\n",
            "정수 인코딩에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n",
            "하위 단어 토큰에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**어휘 사전 불러오기**"
      ],
      "metadata": {
        "id": "EdmV7kEdejL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentencepiece import SentencePieceProcessor\n",
        "\n",
        "\n",
        "tokenizer = SentencePieceProcessor()\n",
        "tokenizer.load(\"../models/petition_bpe.model\")\n",
        "\n",
        "vocab = {idx: tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}      # get_piece_size: 모델에서 생성된 하위 단어 개수를 반환 / id_to_piece: 정수값을 하위 단어로 변환하는 메서드\n",
        "print(list(vocab.items())[:5])\n",
        "print(\"vocab size :\", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V4a7W_keoFv",
        "outputId": "af805720-4dc4-4cde-d3dd-23aa33724463"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '니다'), (4, '▁이')]\n",
            "vocab size : 8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unk는 unknown의 약자로 OOV발생 시 매핑되는 토큰이고 s와 /s 는 문장의 시작점과 종료지점을 표시하는 토큰이다.\n",
        "\n",
        "어휘 사전의 크기는 모델 학습 시 설정한 8000으로 학습 되었고 이 라이브러리는 설정값에 따라 학습 방법이니ㅏ 어휘 사전 크기가 달라질 수 있다."
      ],
      "metadata": {
        "id": "UGOJLPT3flfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 워드피스\n",
        "\n",
        "워드피스 토크나이저는 바이트 페어 인코딩 토크나이저와 유사한 방법으로 학습되지만, 빈도 기반이 아닌 확률 기반으로 글자 쌍을 병합한다.\n",
        "\n",
        "워드피스는 학습 과정에서 확률적인 정보를 사용한다. 모델이 새로운 하위 단어를 생성할 때 이전 하위 단어와 함께 나타날 확률을 계산해 가장 높은 확률을 가진 하위 단어를 선택한다. 이렇게 선택된 하위 단어는 이후에 더 높은 확률로 선택될 가능성이 높으며, 이를 통해 모델이 좀 더 정확한 하위 단어로 분리할 수 있다.\n",
        "\n",
        "$score = \\frac{f(x,y)}{f(x)f(y)}$\n",
        "\n",
        "위 식은 글자 쌍에 대한 점수이다. $f$는 빈도를 나타내는 함수, $x$, $y$병합하고자 하는 하위 단어이다.\n",
        "\n",
        "이 수식을 적용해 어휘 사전 구축 방법을 알아보자.\n",
        "\n",
        "- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o',' w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
        "\n",
        "\n",
        "가장 빈번하게 등장하는 쌍은 9번 등장한 e와 s이다. 하지만 e는 여기서 17번, s는 9번 등장하여 계산을한다면 약 0.06이 된다. 반면, i와 d는 쌍으로는 3번 등장했지만 i 3번, d 3번 하면 약 0.33의 결과가 나오므로 id쌍을 만든다.\n",
        "\n",
        "- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o',' w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'id']\n",
        "\n",
        "동일과정을 반복하여 추가 계산을 해보면 l,o쌍이 0.14로 확률이 높다.\n",
        "\n",
        "- 빈도 사전: ('l', 'o', 'w', 5), ('l', 'o',' w', 'e', 'r', 2), ('n', 'e', 'w', 'e', 's', 't', 6), ('w', 'i', 'd', 'e', 's', 't', 3)\n",
        "- 어휘 사전: ['d', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w', 'id', 'lo']\n",
        "\n",
        "이를 계속 반복해 더이상 연속된 글자 쌍이 나타나지 않을 때까지 시행한다."
      ],
      "metadata": {
        "id": "tLpC1F1H1z49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 토크나이저\n",
        "\n",
        "토크나이저 라이브러리의 워드피스 API를 이용하면 쉽고 빠르게 토크나이저를 구현하고 학습할 수 있다. 이번에는 허깅 페이스의 토크나이저스 라이브러리를 이용해보겠다.\n",
        "\n",
        "**허깅 페이스 토크나이저 라이브러리 설치**\n",
        "\n",
        "```\n",
        "pip install tokenizers\n",
        "```\n",
        "\n",
        "토크나이저 라이브러리는 정규화와 사전 토큰화를 제공한다.\n",
        "\n",
        "정규화는 일관된 형식으로 텍스트를 표준화하고 모호한 경우를 방지하기 위해 일부 문자를 대체하거나 제거하는 등의 작업을 수행한다.\n",
        "\n",
        "사전 토큰화는 입력 문장을 토큰화하기 전에 단어와 같은 작은 단위로 나누는 기능을 제공한다. 공백이나 구두점을 기준으로 입력 문장을 나눠 텍스트 데이터를 효율적으로 처리하고 모델의 성능을 향상 시킬 수 있다."
      ],
      "metadata": {
        "id": "2zMQ4rOl5Dau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(WordPiece())  # 워드피스 모델 불러오기\n",
        "tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n",
        "# Sequence형식으로 정규화 실시 NFD: 유니코드 정규화 / Lowercase: 소문자 변환\n",
        "tokenizer.pre_tokenizer = Whitespace()  # 사전 토큰화 방식 불러오기. 공백, 구두점 기준으로 분리\n",
        "\n",
        "tokenizer.train([\"../datasets/corpus.txt\"])     # 학습\n",
        "tokenizer.save(\"../models/petition_wordpiece.json\")     # 결과 저장"
      ],
      "metadata": {
        "id": "GMvbf_Ql0FiD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**예제16 워드피스 토큰화**"
      ],
      "metadata": {
        "id": "meIrGCrT7d4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer.from_file(\"../models/petition_wordpiece.json\")    # 학습 결과 불러오기\n",
        "tokenizer.decoder = WordPieceDecoder()  #워드피스 디코더로 설정\n",
        "\n",
        "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
        "sentences = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]\n",
        "\n",
        "encoded_sentence = tokenizer.encode(sentence)   # encode로 문장을 토큰화\n",
        "encoded_sentences = tokenizer.encode_batch(sentences)   # 여러문장 토큰화\n",
        "\n",
        "print(\"인코더 형식 :\", type(encoded_sentence))\n",
        "\n",
        "print(\"단일 문장 토큰화 :\", encoded_sentence.tokens)    # tokens속성으로 값 확인 가능\n",
        "print(\"여러 문장 토큰화 :\", [enc.tokens for enc in encoded_sentences])\n",
        "\n",
        "print(\"단일 문장 정수 인코딩 :\", encoded_sentence.ids)      # ids속성으로 문장의 ID출력\n",
        "print(\"여러 문장 정수 인코딩 :\", [enc.ids for enc in encoded_sentences])\n",
        "\n",
        "print(\"정수 인코딩에서 문장 변환 :\", tokenizer.decode(encoded_sentence.ids))    # decode메서드로 정수 인코딩 결과를 다시 문장으로 변환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1WfplwI6Rd5",
        "outputId": "a024cd18-0ac2-4440-a619-a60db8ed2427"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코더 형식 : <class 'tokenizers.Encoding'>\n",
            "단일 문장 토큰화 : ['안녕하세요', ',', '토', '##크', '##나이', '##저', '##가', '잘', '학습', '##되었', '##군요', '!']\n",
            "여러 문장 토큰화 : [['이렇게', '입력', '##값을', '리스트', '##로', '받아서'], ['쉽게', '토', '##크', '##나이', '##저', '##를', '사용할', '수', '있다', '##ᆸ니다']]\n",
            "단일 문장 정수 인코딩 : [8760, 11, 8693, 8415, 16269, 7536, 7488, 7842, 15016, 8670, 8734, 0]\n",
            "여러 문장 정수 인코딩 : [[8187, 19643, 13834, 28119, 7495, 12607], [9739, 8693, 8415, 16269, 7536, 7510, 14129, 7562, 8157, 7489]]\n",
            "정수 인코딩에서 문장 변환 : 안녕하세요, 토크나이저가 잘 학습되었군요!\n"
          ]
        }
      ]
    }
  ]
}